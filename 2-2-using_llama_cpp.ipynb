{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Llama\n",
    "\n",
    "Now that we have some knowledge of the fundamentals of how llama 2 is representing data, both within its model\n",
    "architecture itself as well as the sequences of tokens coming in and out of the model, we can start to query the model.\n",
    "Now, I've used the word \"query\" here, but I don't mean it in the relational database sense, but in the human language\n",
    "sense: to ask a question. A database query is deterministic -- if the data hasn't changed and the query hasn't changed\n",
    "then the same result is always returned. But a language model is a bit more nondeterministic, a more precise word to\n",
    "describe our interaction with the model is \"inference\", or \"predict\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /data/llama-2-13b.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q5_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 8.60 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.18 MiB\n",
      "llm_load_tensors:        CPU buffer size =  8801.63 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   400.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    85.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "# Let's write our first llama.cpp based application.\n",
    "# To create a new model to query we have to identify the quantized\n",
    "# file which we want to use. In this course we have two models created\n",
    "# for experimentation, one being the base 13B paramneter model and the\n",
    "# other being the chat-tuned 13B parameter model. We'll explore the\n",
    "# base model first.\n",
    "\n",
    "# Read in the path for the model file\n",
    "import os\n",
    "\n",
    "model_path: str = os.environ[\"LLAMA_13B\"]\n",
    "\n",
    "# Import the llama.cpp python bindings and load the model\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model: Llama = Llama(model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, all we've done is load the model and llama.cpp has given us a ton of debug information! This is worth taking a quick\n",
    "look at, especially if you might end up using multiple kinds of models, either different architectures or different\n",
    "quantization levels.\n",
    "\n",
    "We see that this is a llama 2 model, that the context length is up to 4096 tokens - and we'll talk about that in a\n",
    "moment - some information about the special tokens which exist for the moment, and if we scroll down we can see the\n",
    "number of layers in the model, which speaks to the internal data structure, and whether these have been configured to be\n",
    "offloaded onto a GPU or not. One of the interesting options in llama.cpp is that you don't have to choose between the\n",
    "CPU or the GPU, you can do a bit of both depending upon your hardware setup.\n",
    "\n",
    "As we get to the bottom here we see the `llama_new_context_with_model` log lines, and this indicates the default seems\n",
    "to be a context of 521 tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '48.4 square miles in size and has a population of around 6', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    }
   ],
   "source": [
    "# Before we go further I want to reduce the verbosity of the output\n",
    "# a bit -- feel free to leave this set to True to see more of the\n",
    "# nitty gritty details of what's happening!\n",
    "model.verbose = False\n",
    "\n",
    "# Now that we have a model let's actually do some inference! The\n",
    "# method we use for this is called create_completion(), and by\n",
    "# default we only need to pass it a prompt to complete and it\n",
    "# returns to us a Completion object, which is a TypedDict.\n",
    "\n",
    "from llama_cpp.llama_types import *\n",
    "\n",
    "result: Completion = model.create_completion(prompt=\"The capital of Michigan is \")\n",
    "\n",
    "# The Completion type has a choices key which shows us the list of\n",
    "# responses the LLM generated, let's take a look\n",
    "print(result[\"choices\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! So, if you are following along in the notebook you undoubtedly have a different response than I'm showing here.\n",
    "That's what we mean by inference, the model is making a series of probabilistic choices based on the input sequence and\n",
    "the weights to generate this output sequence. We can make the model behave more deterministically by setting it's\n",
    "`temperature`, which is a parameter from zero to one where values closer to zero cause the model will behave more\n",
    "deterministically and values closer to one cause the model to behave more non-deterministic, and creative. Let's do a\n",
    "little experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp=0.0, run=0, result: 8 planets, 5 dwarf planets and 123\n",
      "temp=0.0, run=1, result: 8 planets, 5 dwarf planets and 123\n",
      "temp=0.0, run=2, result: 8 planets, 5 dwarf planets and 123\n",
      "temp=0.2, run=0, result: 8 planets and 5 dwarf planets.\n",
      "The planets\n",
      "temp=0.2, run=1, result: 8 planets and 5 dwarf planets.\n",
      "The planets\n",
      "temp=0.2, run=2, result: 8 planets and 5 dwarf planets.\n",
      "The planets\n",
      "temp=0.5, run=0, result: 8 planets and 5 dwarf planets. The largest planet is\n",
      "temp=0.5, run=1, result: 8 major planets, 5 dwarf planets and a number of\n",
      "temp=0.5, run=2, result: 8 planets, 5 dwarf planets and several asteroids.\n",
      "temp=0.75, run=0, result: 8 major planets, i.e., Mercury, Venus, Earth\n",
      "temp=0.75, run=1, result: 8 major planets, 5 dwarf planets and 17\n",
      "temp=0.75, run=2, result: 8 major planets, and these are Mercury, Venus, Earth,\n",
      "temp=1.0, run=0, result: 1) Mercury, 2) Venus, 3) Earth,\n",
      "temp=1.0, run=1, result: 8 planets and their natural satellites. In addition to these, Pl\n",
      "temp=1.0, run=2, result: 8 major and a few minor planets. The outer planets are gas gi\n"
     ]
    }
   ],
   "source": [
    "# Let's try a few different temperature values\n",
    "temps: list[float] = [0.0,0.2, 0.5,0.75, 1.0]\n",
    "\n",
    "# Now, for each of these temperatures, let's do three completions\n",
    "prompt: str = \"The planets in the solar system include \"\n",
    "for temp in temps:\n",
    "    for i in range(0, 3):\n",
    "        result: Completion = model.create_completion(prompt=prompt, temperature=temp)\n",
    "        print(f'temp={temp}, run={i}, result: {result[\"choices\"][0][\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! We can see that the low temperature returns really consistent results, and that the high remperature gives\n",
    "different kinds of answers. Now, don't go reading into the tea leaves too much here, but play with this, experiment and\n",
    "get a feeling for how the temperature effects your prompt completions. Underneath the temperature is changing how the\n",
    "next token is picked from a set of candidate tokens, so higher levels of temperature will deviate more quickly from one\n",
    "another on repeated querying.\n",
    "\n",
    "You've undoubtedly noticed that we're just getting short little responses here. Generating tokens is slow, and by\n",
    "default the `chat_completion()` method limits the number of tokens returned to just 16. We can change this to -1 to\n",
    "generate as many tokens as available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 planets that are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Pluto is also regarded as a planet but recently it has been reclassified as a dwarf planet. There are other bodies too which revolve around the Sun like asteroids and comets.\n",
      "The planets in our solar system can be arranged from the sun in terms of distance, starting with Mercury. It takes 87 days to complete one orbit around the sun. Venus is the second planet after Mercury and it takes 225 days to complete its orbit around the Sun. The next planet is Earth which completes an orbit in 365 days. Mars is the fourth planet from the sun, and it takes approximately 1 year and 320 days to go around the sun. Jupiter has a very long orbital period of almost 12 years and Saturn requires about 29 years. Uranus and Neptune take even longer at about 84 and 165 years, respectively.\n",
      "The planets in our solar system can be grouped according to their physical characteristics too. The inner planets are Mercury, Venus, Earth, Mars and the outer planets include Jupiter, Saturn, Uranus and Neptune. These two groups of planets have many differences between them like size, mass and distance from the sun.\n",
      "The inner planets are rocky whereas outer planets consist mainly of gas with a small solid core in the center. The inner planets also orbit closer to the sun while outer planets have very long orbital periods due to their greater distance. All these factors make it easier for scientists to study these different types of planets.\n",
      "The inner planets are Mercury, Venus, Earth, and Mars. These planets are close to the Sun, meaning they have a short orbital period. The outer planets are Jupiter, Saturn, Uranus, Neptune; they have longer orbits which means it takes them longer to go around once.\n",
      "The inner planets are Mercury, Venus, Earth and Mars. These four rocky worlds orbit close to the sun with short orbital periods (less than one year). The outer planets include Jupiter, Saturn Uranus and Neptune; they have much larger masses than\n"
     ]
    }
   ],
   "source": [
    "# Let's just do one run, and we'll leave the temperature at\n",
    "# it's default value, which is 0.8\n",
    "result: Completion = model.create_completion(prompt=prompt, max_tokens=-1)\n",
    "print(result[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the first thing you'll notice, especially if you are following along in the lab workspaces with me, is that it's\n",
    "slow. We're already running this model in a quantized form, so to speed things up further we need to either get better\n",
    "hardware, or further reduce the model size, perhaps through additional quantization. You'll also likely notice -- and I\n",
    "say likely because everything here is non-deterministic -- that the model still doesn't just \"finish\". It trails off\n",
    "eventually, but not at 16 characters. And this comes down to the content length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Length\n",
    "\n",
    "Once you understand tokenization the context length is really simple, it's just the maximum length of the token sequence\n",
    "that the model is trained on. In the case of the original llama model the context length was just over two thousand\n",
    "tokens (2,024), which means the input data used for training the model was broken up into a maximum this length of\n",
    "sequence. For llama 2 that was increased to over four thousand tokens (4,096).\n",
    "\n",
    "You can think of the context length like the amount of \"memory\" that an LLM has for a given query. The bigger it is the\n",
    "more you can put in the query and thus the more the LLM will be aware of when return to you a response. Training -- and\n",
    "inference -- with a large context length can increase quality of the output, but they do so at the cost of increased\n",
    "computation. When you create a new `Llama()` object in llama.cpp the default maximum context length is set to 512\n",
    "tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "40 lighthouses, over 3,000 miles of coastline on four of the five Great Lakes (Lake Superior, Lake Michigan, Lake Huron and Lake Erie), more than 11,00\n",
      "0 inland lakes, plus countless rivers and streams.\n",
      "Michigan is a state with a humid continental climate. The highest recorded temperature was 112 °F at Mio on July 13, 19\n",
      "36, while the lowest recorded temperature was -51 °F at Vanderbilt on February 9, 1934. Michigan usually has cold winters in the northern/central areas of the state with very warm summers\n",
      " throughout most of the state (especially in the southern portion). The state also experiences a lot of cloud cover and rain throughout the year.\n",
      "A good time to visit the state of Michigan is in the month of July. During this month, the\n",
      " average temperature for Detroit is 74 degrees Fahrenheit and the high temperature is 82 degrees Fahrenheit. The average low temperature is 56 degrees Fahrenheit. There are an average of 10 days with precipitation\n",
      " during July.\n",
      "Detroit is located in the southeastern part of Michigan’s Lower Peninsula. Detroit, known as the “Motor City” or “Town That Put The World On Wheels,” has been a world\n",
      " center for automobile production and design since Henry Ford opened his first plant here in 1903.\n",
      "Detroit is also home to Motown Records, which gave rise to a musical genre of the same name that was popularized by such\n",
      " artists as Diana Ross & The Supremes, Stevie Wonder and Smokey Robinson & The Miracles. The city also boasts numerous museums, parks and recreational areas like Belle Isle Park or Comerica Park,\n",
      " home of Major League Baseball’s Detroit Tigers team.\n",
      "The Henry Ford Museum in Dearborn is a must-see for any visitor to Michigan. Located near the headquarters of Ford Motor Company, it houses an impressive collection of autom\n",
      "obiles and other items from American history. Visitors can also take part in hands-on activities at The Henry Ford’s Greenfield Village, where they will learn about life on a farm or play with vintage toys while listening to\n",
      " stories told by costumed interpreters who bring these experiences alive for modern audiences.\n",
      "The state of Michigan has some fun things to do during your vacation time. You can visit the Henry Ford Museum and Greenfield Village, take in a\n",
      " Tigers game at Comerica Park or just relax on one of its many beautiful beaches."
     ]
    }
   ],
   "source": [
    "# Let's do one last demo, and here I want to show you how we don't have to\n",
    "# wait for whole query to finish, but can instead use the streaming features\n",
    "# of llama.cpp to see tokens as they are completed.\n",
    "\n",
    "# I'm going to create a new model with a nice large context size\n",
    "model: Llama = Llama(model_path=model_path, verbose=False, n_ctx=4096)\n",
    "\n",
    "# If we pass the stream=True parameter to create_completion() we will get back\n",
    "# an iterator of CreatCompletionStreamResponse objects, which are just typed\n",
    "# dictionaries similar to the Completion type\n",
    "token_count: int = 0\n",
    "for result in model.create_completion(\n",
    "    prompt=\"Some fun things to do for vacation in the state of Michigan includes \",\n",
    "    max_tokens=-1,\n",
    "    stream=True,\n",
    "):\n",
    "    # I'm only going to print a newline every 50 tokens or so\n",
    "    if token_count % 50 == 0:\n",
    "        print(\"\")\n",
    "    token_count = token_count + 1\n",
    "    print(result[\"choices\"][0][\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, while you learn a little bit more about all the wonderful things you can do on vacation here in the State of\n",
    "Michigan, I'm going to go grab a diet coke and refresh my voice -- I'll see you in the next video!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
