{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Up until now we've been talking about tokens and text as if they were the same thing and, while similar, it's important\n",
    "to note the differences, and I think we can dive into some llama code to actually explore this. Now, the key issue is\n",
    "that the way in which inference in a neural network, which is the underpinning architecture of large langauage models,\n",
    "works is that it applies mathematical operations to input sequences. In our case case these operations have been learned\n",
    "through pretraining, so the weights in our model get applied to some input sequence of numbers to generate a new output\n",
    "sequence of numbers. So the role of tokenization here is to convert input text into numberic values, and vice versa with\n",
    "the results of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'ml_pytorch (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n ml_pytorch ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Each language model has its own tokenization format, and we can load and play with\n",
    "# llama2's tokenizer by loading it directly\n",
    "from llama import Tokenizer\n",
    "\n",
    "tok = Tokenizer(\"tokenizer.model\")\n",
    "\n",
    "# We can take a look at the size of the vocabulary, in the case of llama2 that vocabulary\n",
    "# is 32,000 tokens large\n",
    "tok.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can represent a lot of things with 32,000 tokens, and one might think this is plenty and the approach would be\n",
    "to tokenize every single character in the text and use the model to predict the next character as output. This doesn't\n",
    "work well. The approach taken instead is to tokenize common words in the corpus of training data -- and this is why a\n",
    "diverse and representative set of training data is needed -- and add to it \"backups\" for those words we don't see very\n",
    "often, by including in tokens for individual characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the encode() function to generate a sequence of token numbers for\n",
    "# a given sentance. There are a few special tokens we need to be aware of,\n",
    "# specifically the bos (beginning of sequence) and eos (end of sequence) tokens.\n",
    "# We can control whether we want these to appear in our sequence or not through\n",
    "# the ecode bos and eos parameters.\n",
    "tok.encode(\"Llama is an animal.\", bos=True, eos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case of this sentence, we can see that in addition to the bos and eos tokens, seven other tokens were generated.\n",
    "Perhaps a bit mysterious, as there are only four words in the sentenance. Let's unpack each token and see how the llama\n",
    "tokenizer worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll just iterate over each token and as the tokenizer to decode() it.\n",
    "for token in tok.encode(\"Llama is an animal.\", bos=True, eos=True):\n",
    "    print(f\"Token {token} is {tok.decode(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the start and end tokens have no visible representation, and that seems reasonable. Suprisingly, the\n",
    "word Llama isn't in the vocabulary, and instead the tokenizer has fallen back to spelling it out letter by letter until\n",
    "it hits a segment which is in the vocabulary (in this case, \"ama\"). We see that the words, \"is\", \"an\", and \"animal\" are\n",
    "all in the vocabulary, as well as the full stop or period. Let's play with this and interrogate the tokenizer a little\n",
    "more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's our original sentence\n",
    "original = tok.encode(\"Llama is an animal.\", bos=True, eos=True)\n",
    "print(original)\n",
    "print(f\"{[tok.decode(token) for token in original]}\")\n",
    "\n",
    "# Let's see what changes if we make it all lower case\n",
    "lower = tok.encode(\"llama is an animal.\", bos=True, eos=True)\n",
    "print(lower)\n",
    "print(f\"{[tok.decode(token) for token in lower]}\")\n",
    "\n",
    "# And let's throw in a misspelling\n",
    "misspelt = tok.encode(\"llama is an aminal.\", bos=True, eos=True)\n",
    "print(misspelt)\n",
    "print(f\"{[tok.decode(token) for token in misspelt]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! So, spelling matters, as does capitalization. We see that the word llama still doesn't appear directly in the\n",
    "vocabulary, but that the tokenization of the lower case word and the sentence case words of llama result in not only\n",
    "different sequences but a different length of those sequences. We also see that a misspelling of the word \"animal\"\n",
    "results in two tokens being returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's also useful to take a look at numbers, given that tokens are represented\n",
    "# as numbers and LLMs have historically been really bad at solving math problems!\n",
    "# I'm going to not put in the BOS and EOS tokens anymore, but don't forget they\n",
    "# are there for your prompts.\n",
    "eq = tok.encode(\"11+23=\", bos=False, eos=False)\n",
    "print(eq)\n",
    "print(f\"{[tok.decode(token) for token in eq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this can be a bit perplexing, as there is a token right at the beginning of the sequence -- 29871 -- which actually\n",
    "is a special token put in by the tokenizer, and represents a start of word whitespace. Ignoring that for the moment,\n",
    "it's important for you to understand that the tokenizer has no symbolic understanding of numbers -- the number eleven is\n",
    "simply a one followed by another one.\n",
    "\n",
    "Understanding this process at a high level is important, because our language model doesn't operate on words or\n",
    "sentences or even text -- it operates on sequences of tokens. This is important when understanding more advanced\n",
    "concepts in using an LLM, such as Retrieval Augmented Generation (RAG), where the LLM has access to a database of\n",
    "documents all indexed by chunks. These chunks are tokens which are then expressed as vectors and, the details aren't\n",
    "important right now, but it is important to know that tokens (and sequences of them) are the fundamental unit of input\n",
    "and output of a language model.\n",
    "\n",
    "It also lets us talk about something else important: Context length, and we'll do that by writing some prompts in the\n",
    "next lecture.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
