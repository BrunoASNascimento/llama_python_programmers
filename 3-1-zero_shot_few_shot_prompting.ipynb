{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero and Few Shot Prompting\n",
    "\n",
    "Writing good prompts for large language models is a combination of art, science and, perhaps a bit of wizardry. As you\n",
    "know, the model is a statistical one, taking numeric input sequences and creating an output sequence to match based on\n",
    "the (a) pretrained data, (b) the input sequence itself, and (c) the various parameters we can use when calling the\n",
    "model. In this lecture I want to introduce you to three different prompting strategies. I caution that there are many\n",
    "more strategies out there, but these two form a nice foundation for current practices and they work well with llama 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Prompting\n",
    "\n",
    "You've actually seen zero shot prompting many times in this course -- it's simply giving the model a single set of\n",
    "instructions to respond to, and letting the model use it's pretrained weights. And actually, calling the inputs as\n",
    "\"instructions\" is questionable at best -- many of us are use to the chat gpt conversational style, but the model is\n",
    "actually just doing output sequence prediction. So zero shot prompting is giving an input sequence and taking from the\n",
    "model a predicted appropriate output sequence.\n",
    "\n",
    "Throughout this lecture I'm going to use a really authentic task for myself -- you see, I teach a lot of programming in\n",
    "my day to day job at the University of Michigan and here on the Coursera platform. This often coveres topics in the\n",
    "areas of python, data science, and applied AI. One of the things I'd like to do is give students more quick questions to\n",
    "test their knowledge. But, coming up with good questions is difficult, and then I have to type them all out and enter\n",
    "them into a quizzing tool to be delivered to the learner. Most of these tools can take JSON formatted questions, but I\n",
    "find typing out JSON documents as slow and error prone. So let's see if we can build a conversational agent to help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's setup our lamma model, and this time I'm going to bump up the context\n",
    "# window a bit. This can slow things down, but also will result in more output\n",
    "# tokens being sent back to us.\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_types import *\n",
    "\n",
    "model: Llama = Llama(\n",
    "    model_path=os.environ[\"LLAMA_13B\"], verbose=False, n_ctx=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " how can I do this?\n",
      "\n",
      "I am new to Python and I have a question about the lambda function.\n",
      "In my code, I need to do something like:\n",
      "\n",
      "\\begin{code}\n",
      "def parse_json(in_json):\n",
      "    print json.dumps({\"a\": {lambda x: x[0]}}, indent=2)\n",
      "\n",
      "parse_json('[\"a\"]')\n",
      "\\end{code}\n",
      "\n",
      "The output is:\n",
      "\n",
      "\\begin{code}\n",
      "[{'a': <function __main__.lambda at 0x13fc8e75>}]\n",
      "\\end{code}\n",
      "\n",
      "How to get the desired result? I want \"a\": 0.\n",
      "\n",
      "Comment: Please provide a [mcve]\n",
      "\n",
      "Answer: You need to convert it to string:\n",
      "\n",
      "\\begin{code}\n",
      "print json.dumps({\"a\": lambda x: str(x[0])}, indent=2)\n",
      "\\end{code}"
     ]
    }
   ],
   "source": [
    "# Now, in zero shot prompting we're just asking the language model to\n",
    "# continue our text it's pretrained weights. Since I want to generate\n",
    "# some python 3 lambda questions in JSON, this seems like a good\n",
    "# starting point!\n",
    "\n",
    "prompt = \"Python 3 lambda question in JSON:\"\n",
    "\n",
    "# Now let's watch the results. Remember you need to increase the\n",
    "# max_tokens as well as the context window or llama.cpp will cut\n",
    "# off the reply\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, well, that doesn't seem like a completely unreasonable response to the prompt, but it's certainly not what I was\n",
    "looking for. Let's try another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97% humanly readable.\n",
      "(That is, if you ignore the fact that you would have to be a Python programmer to even understand what it's trying to say.)\n",
      "\"This is one of my favourite examples for illustrating how human-readable Python code can be.\" â€” Richard Barrow, creator of the Bloggify project.\n",
      "The resultant JSON output is rendered in HTML using the beautiful syntax highlighting and other features provided by Prism:\n",
      "https://github.com/prismjs/prism"
     ]
    }
   ],
   "source": [
    "# Just a little tweak, trying to write the prompt as if it were\n",
    "# something that was observed in the training data, e.g. a textbook\n",
    "prompt = \"A good Python 3 lambda question rendered in JSON is \"\n",
    "\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompting\n",
    "\n",
    "Large language models are statistical pattern matching machines, and the idea behind few-shot prompting is that we can\n",
    "give examples in our prompt to help the model tailor its output to what we are looking for. This turns out to be a sort\n",
    "of very simple super power for prompt engineering, and is very helpful when you want to constrain the responses from a\n",
    "model to a specific format. Let's see if it helps us here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm increasing the size of my prompt, but I'm going back to the format I had\n",
    "# previously. I've intentionally done two things here: (a) a macro pattern, where\n",
    "# I indicate I'm looking for a question in JSON and I just vary the topic, and\n",
    "# (b) a format pattern, where I show what I want the output to look like.\n",
    "\n",
    "prompt = \"\"\"Python 3 lambda question in JSON:\n",
    "{\"question\":\"The lambda keyword in python is:\",\"correct_answer\":\"For declaring anonymous functions\",\"incorrect_answer\":\"For mathematical operations\"}\n",
    "\n",
    "Python 3 def question in JSON:\n",
    "{\"question\":\"What does the 'def' keyword do?\",\"correct_answer\":\"Define a function\",\"incorrect_answer\":\"Declare variables\"}\n",
    "\n",
    "Python 3 assert question in JSON: \n",
    "\"\"\"\n",
    "\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. Instantly we get basically what I was looking for -- a question on the topic of asserts in python 3. Sometimes when\n",
    "I run this I also get a number of other questions, with the model just continuing the pattern and going through a list\n",
    "of python keywords and topics and giving me output. I don't always want this, as I don't cover all of the topics it\n",
    "might generate text for. Let's tweak this a bit more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I'm just adding the topics at the very beginning. I expect that the model\n",
    "# is going to recognize the patterns here, seeing the list of topics, are repeated\n",
    "# in the individual prompts, and that it will follow and just give me results\n",
    "# for the python 3 assert, with, and import keywords.\n",
    "prompt = \"\"\"Topics: lambda, def, assert, with, import.\n",
    "\n",
    "Python 3 lambda question in JSON:\n",
    "{\"question\":\"The lambda keyword in python is:\",\"correct_answer\":\"For declaring anonymous functions\",\"incorrect_answer\":\"For mathematical operations\"}\n",
    "\n",
    "Python 3 def question in JSON:\n",
    "{\"question\":\"What does the 'def' keyword do?\",\"correct_answer\":\"Define a function\",\"incorrect_answer\":\"Declare variables\"}\n",
    "\n",
    "Python 3 assert question in JSON: \n",
    "\"\"\"\n",
    "\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, things are getting exciting and I've just about automated my weekend job! I actually just want the JSON\n",
    "results, and sometimes (though maybe not in the case in your notebook if you are following along!) the result doesn't\n",
    "have all of the JSON fields I might want. Remember, everything is tokens and sequences, and the input prompt is a\n",
    "statistical machine, so there are a couple of things we might tweak further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to include the list of topics at the top, then I'm going to\n",
    "# use some whitespace formatting on the JSON with newlines to see if this\n",
    "# helps increase adherence to the format while keeping the semantics.\n",
    "# everything except the first line of my\n",
    "\n",
    "prompt = \"\"\"\n",
    "{\"python_3_topics\" = [\"lambda\", \"def\", \"assert\", \"with\", \"import\"],questions=[\n",
    "{\n",
    "\"question\":\"The lambda keyword in python is:\",\n",
    "\"correct_answer\":\"For declaring anonymous functions\",\n",
    "\"incorrect_answer\":\"For mathematical operations\"\n",
    "},\n",
    "{\n",
    "\"question\":\"What does the 'def' keyword do?\",\n",
    "\"correct_answer\":\"Define a function\",\n",
    "\"incorrect_answer\":\"Declare variables\"\n",
    "},\n",
    "{\n",
    "\"\"\"\n",
    "\n",
    "for response in model.create_completion(prompt, max_tokens=2048, stream=True):\n",
    "    result = response[\"choices\"][0]\n",
    "    print(result[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I think that's pretty solid. What's really cool about this, I think, is that I'm not conversing with the model, or\n",
    "trying to prime it to be an expert. I just started a JSON document and it captured both the meaning of what I was doing\n",
    "-- writing questions with correct and incorrect answers -- and the syntax of what I was doing -- writing well formed\n",
    "JSON. All this on a quantized 13B parameter model!\n",
    "\n",
    "This is a great time to jump into the notebooks and experiment a bit yourself to see this in action. Here are a couple\n",
    "of nice tasks for you to try and practice what you've learned; first, how would you reimplement this code using the\n",
    "llama 2 chat model?, and second, how would you rewrite the prompts so that there were multiple incorrect answers, all in\n",
    "a JSON list of their own? Give these a shot in the labs workspace.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
