{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 2 Chat and Conversations\n",
    "\n",
    "We now know how to load llama2 as a local large language model, that fundamentally it takes a sequence of tokens and\n",
    "uses it's pretrained weights to generate a sequence of output tokens, and that this process is probabalistic in nature\n",
    "and we have some techniques to help guide that randomness. Remember, these pretrained weights come from a large corpus\n",
    "of data -- over 2 trillion tokens -- and how this data was fed into the model determines the weights, representing,\n",
    "roughly, the relationship between tokens. It's important then to understand how that data was formatted when it was fed\n",
    "into the model in order to generate reasonable prompts. The base model which we've been using up until this point\n",
    "doesn't have any particular template, but the chat model does as it went through the fine tuning process.\n",
    "\n",
    "From the llama 2 paper, the template is as follows:\n",
    "\n",
    "`<s>[INST] <<SYS>>\\n{your_system_message}\\n<</SYS>>\\n\\n{user_message_1} [/INST] {{ llama_answer_1 }} </s>`\n",
    "\n",
    "First, the sequence starts with the letter s enclosed in less than and greater than symbols. Then the string [INST]\n",
    "appears, indicating that a single instruction is being given. You can see this is mirrored at the end of the string with\n",
    "a [/INST], indicating the closing of the instruction. Then we have the <<SYS>> indicator inside a set of ASCII\n",
    "guillemets, indicating that the prompt begins with a system message to set the context. There is a variety of whitespace\n",
    "-- which is important because it was in the fine tuned data -- and a user message which is the actual content of your\n",
    "prompt query. The system will append to this it's response to the query, and future queries in the same conversation\n",
    "should be user messages wrapped in the [INST][/INST] tags.\n",
    "\n",
    "This can be a lot to keep in mind, but llama.cpp and its python bindings help wrap this format a bit for us, let's take\n",
    "a look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the path for the model file, note the new environment variable!\n",
    "# Also, by default llama.cpp sets the temperature of our model down to 0.2\n",
    "# for chatting, but we can override that and make it whatever we want.\n",
    "# Here I'll put zero, as I want to get pretty repeatable accurate results.\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_types import *\n",
    "\n",
    "model: Llama = Llama(\n",
    "    model_path=os.environ[\"LLAMA_13B_CHAT\"], verbose=False, temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-a93e53a1-698a-42c7-821b-ac7d3838806b\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1718484598,\n",
      "    \"model\": \"/data/llama-2-13b-chat.Q5_K_M.gguf\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"  The fifth planet in the solar system is Earth.\"\n",
      "            },\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 62,\n",
      "        \"completion_tokens\": 11,\n",
      "        \"total_tokens\": 73\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# The high level type llama.cpp uses for this is a type dictionary which\n",
    "# is called ChatCompletionMessage. This dictionary will be converted at\n",
    "# inference time to the underlying LLM format you need, based on the model type\n",
    "\n",
    "# Also, since everything is a typed dictionary in the llama.cpp bindings,\n",
    "# I'm going to bring in a json helper library for improving display\n",
    "import json\n",
    "\n",
    "# This will be our conversation histor, our messages\n",
    "messages = []\n",
    "\n",
    "# Let's first add a system message, indicating that only a single word should be used in the response\n",
    "message = ChatCompletionMessage()\n",
    "message[\"role\"] = \"system\"\n",
    "message[\"content\"] = (\n",
    "    'You are an expert astronomer, answer each question which is given to you as factually as possible. If you do not know the answer say \"unknown\".'\n",
    ")\n",
    "messages.append(message)\n",
    "\n",
    "# Now let's add a user query\n",
    "message = ChatCompletionMessage()\n",
    "message[\"role\"] = \"user\"\n",
    "message[\"content\"] = \"What is the fifth planet in the solar system?\"\n",
    "messages.append(message)\n",
    "\n",
    "# Now let's see the result\n",
    "result = model.create_chat_completion(messages=messages)\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! So we can see we get a UUID it looks like back to the chat, as well as which model was used then the list of choices\n",
    "which is just the assistant saying the word \"Earth\".\n",
    "\n",
    "Earth!? I know we got rid of Pluto since I was a kid, but did we add a few more? Well, let's give the model a pass on\n",
    "accuracy for the moment.\n",
    "\n",
    "How do we know what our message actually looked like, and what kind of roles are available to us?\n",
    "\n",
    "Well, this comes down to abstractions -- see llama.cpp isn't just for llama 2, and each model that's created has its own\n",
    "chat format depending on how it was finetuned. Despite models seeming similar in how we use them, there is potential for\n",
    "huge variation, and llama.cpp has been abstracted to work with a number of models, including other open source models\n",
    "like Minstral. Not only does each model have it's own vocabulary, model architecture, pre-trained weights, and fine\n",
    "tuned message format, some don't just include text as sequence input, and have multimodal capabilities, bringing in\n",
    "images and video. It's a pretty exciting time!\n",
    "\n",
    "But back to llama 2 here, if we want to see how the message is being translated into text we can take a look at the chat\n",
    "formatters available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are an expert astronomer, answer each question which is given to you as factually as possible. If you do not know the answer say \"unknown\".\n",
      "<</SYS>> What is the fifth planet in the solar system? [/INST]\n"
     ]
    }
   ],
   "source": [
    "# The chat format is automatically loaded when we load the model, we\n",
    "# can see what the format is with this variable\n",
    "model.chat_format  # this will be the str llama-2 in our example\n",
    "\n",
    "# We can actually load and call the prompt formatting function\n",
    "# ourselves if we would like to\n",
    "from llama_cpp.llama_chat_format import format_llama2, ChatFormatterResponse\n",
    "\n",
    "response: ChatFormatterResponse = format_llama2(messages=messages)\n",
    "print(response.prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so this looks just like the format I introduced you to, but of course the newline characters are rendered as returns\n",
    "in the document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning a Chat into a Conversation\n",
    "\n",
    "One of the real powers of the chat module is that we can have multi turn conversations. Large language models work with\n",
    "a set context window, and we've already discussed that with llama 2 this context window is up to 4,096 tokens large, but\n",
    "by default llama.cpp creates a context window of 512 tokens to save some computational resources. The most\n",
    "straightforward conversation for a llama 2 model then is just a series of messages that we append all together into one\n",
    "larger sequence, iteratively adding in the responses from the model through turns. Let's take a look at an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"  You are correct, Earth is the third planet in our solar system, not the fifth. The order of the planets in our solar system, starting from the Sun and moving outward, is:\\n\\n1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n\\nSo, there is no planet after Earth in terms of distance from the Sun.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# I'll create a list here to hold the messages\n",
    "messages = []\n",
    "\n",
    "# Each message is a ChatCompletionMessage object, which again, is just a typed\n",
    "# dictionary. The first message will be the system context.\n",
    "message = ChatCompletionMessage()\n",
    "message[\"role\"] = \"system\"\n",
    "message[\"content\"] = (\n",
    "    'You are an expert astronomer, answer each question which is given to you as factually as possible. If you do not know the answer say \"unknown\".'\n",
    ")\n",
    "messages.append(message)\n",
    "\n",
    "# Now let's add a user query\n",
    "message = ChatCompletionMessage()\n",
    "message[\"role\"] = \"user\"\n",
    "message[\"content\"] = \"What is the fifth planet in the solar system?\"\n",
    "messages.append(message)\n",
    "\n",
    "# Now let's get the result\n",
    "result = model.create_chat_completion(messages=messages)\n",
    "\n",
    "# We're actually not even going to look at the result, and assume it is wrong,\n",
    "# so let's add that to our list of messages. In llama 2 we get back a typed\n",
    "# dict where the message is the first choice\n",
    "messages.append(result[\"choices\"][0][\"message\"])\n",
    "\n",
    "# Now let's ask for a clarification\n",
    "message = ChatCompletionMessage()\n",
    "message[\"role\"] = \"user\"\n",
    "message[\"content\"] = (\n",
    "    \"Are you certain that is correct? I thought Earth was the third planet in our solar system. Which planet comes after Earth in distance from the Sun?\"\n",
    ")\n",
    "messages.append(message)\n",
    "\n",
    "# Let's see how we do\n",
    "result = model.create_chat_completion(messages=messages)[\"choices\"][0][\"message\"]\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I'm a bit disappointed in this result, but keep in mind we're taking a smaller model -- the 13 billion parameter\n",
    "model -- and then quantizing it heavily to run on the CPU on the Coursera web environment for this class. A less\n",
    "compressed -- that is, less quantized -- model may perform better, as might a larger model. Or we could try and tune our\n",
    "prompt a bit more through some prompt engineering strategies.\n",
    "\n",
    "An actually, that's kind of a nice way to end this module. We've gotten our hands dirty with llama 2 and llama.cpp using\n",
    "python. We've explored how tokenization works, and the parameters by which we can tune how this probabilistic model\n",
    "actually chooses a new output token. We've also seen that different models can have different kinds of interaction norms\n",
    "depending on how they have been fine tuned, with llama 2 chat being a good example of a pretty simple but important\n",
    "template. And, we've seen in this template that there are three kinds of roles that exist for messages -- a system\n",
    "message, which is optional and sets the norm for how the model should respond, then any number of user and assistant\n",
    "messages, representing turns in a dialog.\n",
    "\n",
    "But now I'd like to learn from you, how might you tackle improving the response from llama 2 on this question? You don't\n",
    "have to limit yourself to just the concepts we've talked about here so far -- feel free to bring in knowledge you might\n",
    "have from other videos or readings you might have seen. And, feel free to actually try and improve the results for this\n",
    "query in the notebooks associate with the course.\n",
    "\n",
    "In the next module, we'll look a bit more at how we can programatically integrate with large language models, and I'll\n",
    "give you a short practice assignment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
